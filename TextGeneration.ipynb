{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', 'TextGeneration.ipynb', 'HPBook1.txt', 'final_model']\n"
     ]
    }
   ],
   "source": [
    "#Libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import os\n",
    "from collections import Counter\n",
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('./HPBook1.txt', encoding = 'utf-8')\n",
    "file_contents = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample text from book\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Harry Potter and the Sorcerer's Stone \\n\\nCHAPTER ONE \\n\\nTHE BOY WHO LIVED \\n\\nMr. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much. They were the last people you'd expect to be involved in anything strange or mysterious, because they just didn't hold with such nonsense. \\n\\nMr. Dursley was the director of a firm called Grunnings, which made drills. He was a big, beefy man with hardly any neck, although he did have a very large mustache. Mrs. Dursley was thin and blonde and had nearly twice the usual amount of neck, which came in very useful as she spent so much of her time craning over garden fences, spying on the neighbors. The Dursleys had a small son called Dudley and in their opinion there was no finer boy anywhere. \\n\\nThe Dursleys had everything they wanted, but they also had a secret, and their greatest fear was that somebody would discover it. They didn't think they could bear it if anyone found out about the Potters\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Sample text from book')\n",
    "file_contents[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in book : 78449\n",
      "Total characters in book : 442744\n",
      "Unique words in book : 11897\n",
      "Unique characters in book : 80\n",
      "Total Paragraphs : 3033\n"
     ]
    }
   ],
   "source": [
    "total_words = len(file_contents.split())\n",
    "total_characters = len(file_contents)\n",
    "unique_words = len(set(file_contents.split()))\n",
    "unique_characters = len(set(file_contents))\n",
    "paragraphs = file_contents.split('\\n\\n')\n",
    "\n",
    "print (\"Total words in book :\", total_words)\n",
    "print (\"Total characters in book :\", total_characters)\n",
    "print (\"Unique words in book :\", unique_words)\n",
    "print (\"Unique characters in book :\", unique_characters)\n",
    "print (\"Total Paragraphs :\", len(paragraphs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' ', 78449),\n",
       " ('e', 39628),\n",
       " ('t', 27993),\n",
       " ('a', 25887),\n",
       " ('o', 25809),\n",
       " ('n', 21337),\n",
       " ('r', 20990),\n",
       " ('h', 19535),\n",
       " ('i', 19422),\n",
       " ('s', 18870),\n",
       " ('d', 15932),\n",
       " ('l', 14385),\n",
       " ('u', 9562),\n",
       " ('y', 8293),\n",
       " ('g', 8127),\n",
       " ('w', 7744),\n",
       " ('m', 6729),\n",
       " ('f', 6431),\n",
       " ('c', 6403),\n",
       " ('.', 6136),\n",
       " (',', 5658),\n",
       " ('b', 4980),\n",
       " ('p', 4909),\n",
       " ('\"', 4747),\n",
       " ('k', 3930),\n",
       " (\"'\", 3141),\n",
       " ('H', 2996),\n",
       " ('v', 2716),\n",
       " ('-', 1986),\n",
       " ('I', 1393),\n",
       " ('T', 1055),\n",
       " ('S', 844),\n",
       " ('?', 754),\n",
       " ('A', 703),\n",
       " ('D', 685),\n",
       " ('M', 665),\n",
       " ('R', 660),\n",
       " ('W', 653),\n",
       " ('P', 639),\n",
       " ('G', 492),\n",
       " ('N', 488),\n",
       " ('!', 474),\n",
       " ('F', 426),\n",
       " ('x', 381),\n",
       " ('B', 348),\n",
       " ('O', 332),\n",
       " ('Y', 326),\n",
       " ('j', 319),\n",
       " ('C', 293),\n",
       " ('E', 287),\n",
       " ('z', 259),\n",
       " ('q', 217),\n",
       " ('L', 209),\n",
       " ('Q', 203),\n",
       " ('U', 193),\n",
       " ('V', 192),\n",
       " (';', 135),\n",
       " ('K', 79),\n",
       " (':', 69),\n",
       " ('J', 51),\n",
       " (')', 33),\n",
       " ('(', 30),\n",
       " ('“', 11),\n",
       " ('1', 11),\n",
       " ('3', 8),\n",
       " ('4', 6),\n",
       " ('Z', 5),\n",
       " ('0', 5),\n",
       " ('7', 4),\n",
       " ('9', 4),\n",
       " ('2', 3),\n",
       " ('X', 2),\n",
       " ('5', 2),\n",
       " ('*', 2),\n",
       " ('–', 1),\n",
       " ('~', 1),\n",
       " ('8', 1),\n",
       " ('6', 1),\n",
       " ('\\\\', 1)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_words = Counter()\n",
    "for i in range(len(paragraphs)):\n",
    "    for x in paragraphs[i]:\n",
    "        most_words[x] +=1\n",
    "\n",
    "most_words.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n': 0,\n",
       " ' ': 1,\n",
       " '!': 2,\n",
       " '\"': 3,\n",
       " \"'\": 4,\n",
       " '(': 5,\n",
       " ')': 6,\n",
       " '*': 7,\n",
       " ',': 8,\n",
       " '-': 9,\n",
       " '.': 10,\n",
       " '0': 11,\n",
       " '1': 12,\n",
       " '2': 13,\n",
       " '3': 14,\n",
       " '4': 15,\n",
       " '5': 16,\n",
       " '6': 17,\n",
       " '7': 18,\n",
       " '8': 19,\n",
       " '9': 20,\n",
       " ':': 21,\n",
       " ';': 22,\n",
       " '?': 23,\n",
       " 'A': 24,\n",
       " 'B': 25,\n",
       " 'C': 26,\n",
       " 'D': 27,\n",
       " 'E': 28,\n",
       " 'F': 29,\n",
       " 'G': 30,\n",
       " 'H': 31,\n",
       " 'I': 32,\n",
       " 'J': 33,\n",
       " 'K': 34,\n",
       " 'L': 35,\n",
       " 'M': 36,\n",
       " 'N': 37,\n",
       " 'O': 38,\n",
       " 'P': 39,\n",
       " 'Q': 40,\n",
       " 'R': 41,\n",
       " 'S': 42,\n",
       " 'T': 43,\n",
       " 'U': 44,\n",
       " 'V': 45,\n",
       " 'W': 46,\n",
       " 'X': 47,\n",
       " 'Y': 48,\n",
       " 'Z': 49,\n",
       " '\\\\': 50,\n",
       " 'a': 51,\n",
       " 'b': 52,\n",
       " 'c': 53,\n",
       " 'd': 54,\n",
       " 'e': 55,\n",
       " 'f': 56,\n",
       " 'g': 57,\n",
       " 'h': 58,\n",
       " 'i': 59,\n",
       " 'j': 60,\n",
       " 'k': 61,\n",
       " 'l': 62,\n",
       " 'm': 63,\n",
       " 'n': 64,\n",
       " 'o': 65,\n",
       " 'p': 66,\n",
       " 'q': 67,\n",
       " 'r': 68,\n",
       " 's': 69,\n",
       " 't': 70,\n",
       " 'u': 71,\n",
       " 'v': 72,\n",
       " 'w': 73,\n",
       " 'x': 74,\n",
       " 'y': 75,\n",
       " 'z': 76,\n",
       " '~': 77,\n",
       " '–': 78,\n",
       " '“': 79}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = sorted(set(file_contents))\n",
    "vocab_to_int = {c : i for i,c in enumerate(vocab)}\n",
    "int_to_vocab = dict(enumerate(vocab))\n",
    "embeddings = np.array([vocab_to_int[i] for i in file_contents], dtype=np.int32)\n",
    "\n",
    "vocab_to_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples encodings\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Harry Potter and the Sorcerer's Stone \n",
      "\n",
      "CHAPTER ONE \n",
      "\n",
      "THE BOY WHO LIVED \n",
      "\n",
      "Mr. and Mrs. Dursley, of number four, Privet D\n",
      "[31 51 68 68 75  1 39 65 70 70 55 68  1 51 64 54  1 70 58 55  1 42 65 68\n",
      " 53 55 68 55 68  4 69  1 42 70 65 64 55  1  0  0 26 31 24 39 43 28 41  1\n",
      " 38 37 28  1  0  0 43 31 28  1 25 38 48  1 46 31 38  1 35 32 45 28 27  1\n",
      "  0  0 36 68 10  1 51 64 54  1 36 68 69 10  1 27 71 68 69 62 55 75  8  1\n",
      " 65 56  1 64 71 63 52 55 68  1 56 65 71 68  8  1 39 68 59 72 55 70  1 27]\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print (\"Examples encodings\")\n",
    "print('-'*100)\n",
    "print (file_contents[:120])\n",
    "print (embeddings[:120])\n",
    "print('-'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(inputs, batch_size, num_steps):\n",
    "    char_batch = batch_size * num_steps\n",
    "    num_batches = len(inputs)//char_batch\n",
    "    \n",
    "    idx = char_batch * num_batches \n",
    "    inputs = inputs[:idx]\n",
    "    inputs = inputs.reshape((batch_size, -1))\n",
    "    \n",
    "    for i in range(0, inputs.shape[1], num_steps):\n",
    "        x = inputs[:, i : i+num_steps]\n",
    "        y = np.zeros_like(x)\n",
    "        y[:, :-1], y[:, -1] = x[:, 1:], x[:, 0]\n",
    "        \n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class charLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, num_emb, emb_dim, hidden_dim, tagset_size):\n",
    "        super(charLSTM, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_emb = num_emb\n",
    "        self.emb_dim = emb_dim\n",
    "        \n",
    "        self.word_embeddings = nn.Embedding(num_emb, emb_dim)\n",
    "\n",
    "        self.lstm = nn.LSTM(emb_dim, hidden_dim, 2, batch_first = True)\n",
    "\n",
    "        self.hidden2out = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, hidden = self.lstm(embeds)\n",
    "        out_space = self.hidden2out(lstm_out)\n",
    "        out_scores = F.log_softmax(out_space, dim = 2)\n",
    "        \n",
    "        return out_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_emb = len(int_to_vocab)\n",
    "emb_dim = 16\n",
    "hidden_dim = 512\n",
    "target_size = num_emb\n",
    "model = charLSTM(num_emb, emb_dim, hidden_dim, target_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "print_every = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = generate_batches(embeddings, batch_size, 100)\n",
    "batches = list(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = np.array(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(138, 2, 32, 100)\n"
     ]
    }
   ],
   "source": [
    "print(batches.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------\n",
    "#MODEL TRAINING\n",
    "#------------------------------------------------------------\n",
    "\n",
    "\n",
    "# for epoch in range(100):\n",
    "#     print('-'*50)\n",
    "#     print('NEW EPOCH:',epoch+1)\n",
    "#     print('-'*50)\n",
    "    \n",
    "#     count = 0\n",
    "    \n",
    "#     for i in range(len(batches)):\n",
    "        \n",
    "#         batch = batches[i]\n",
    "#         batch_x = batch[0]\n",
    "#         batch_y = batch[1]\n",
    "#         batch_x = torch.tensor(batch_x, dtype = torch.long)\n",
    "#         batch_y = torch.tensor(batch_y, dtype = torch.long)\n",
    "        \n",
    "#         softmax = model(batch_x)\n",
    "#         loss = loss_function(softmax.view(-1,num_emb),batch_y.view(-1))\n",
    "        \n",
    "#         loss.backward()\n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), .25)\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         count += batch_size\n",
    "        \n",
    "#         if(count%print_every == 0):\n",
    "#             print(loss,count,sep = ':')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------\n",
    "#MODEL SAVING\n",
    "#------------------------------------------------------------\n",
    "\n",
    "# torch.save(model.state_dict(), './modelParams')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#------------------------------------------------------------\n",
    "#MODEL LOADING\n",
    "#------------------------------------------------------------\n",
    "\n",
    "model = charLSTM(num_emb, emb_dim, hidden_dim, target_size)\n",
    "model.load_state_dict(torch.load('./final_model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateSentence(starting_text, length):\n",
    "\n",
    "    text = [w for w in starting_text]\n",
    "\n",
    "    tensors = [vocab_to_int[t] for t in text]\n",
    "    ten = torch.tensor(tensors, dtype = torch.long).view(1,-1)    \n",
    "\n",
    "    last = model(ten)[0][-1]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        print(starting_text, end = '')\n",
    "        prediction = last.argmax().tolist()\n",
    "        print(int_to_vocab[prediction],end = '')\n",
    "        text.append(int_to_vocab[prediction])\n",
    "        tensors = [vocab_to_int[t] for t in text]\n",
    "        ten = torch.tensor(tensors, dtype = torch.long).view(1,-1)    \n",
    "        emb = model.word_embeddings(ten)\n",
    "        lstm_out, hidden = model.lstm(emb)\n",
    "        hidden_out = model.hidden2out(lstm_out)\n",
    "        softmax_out = F.log_softmax(hidden_out, dim = 2)\n",
    "        softmax_out = softmax_out[0][-1]\n",
    "        prediction = softmax_out.argmax().tolist()\n",
    "        print(int_to_vocab[prediction],end = '')\n",
    "\n",
    "        for i in range(length):\n",
    "            text = [int_to_vocab[prediction]]\n",
    "            tensors = [vocab_to_int[t] for t in text]\n",
    "            ten = torch.tensor(tensors, dtype = torch.long).view(1,-1)  \n",
    "            emb = model.word_embeddings(ten)\n",
    "            lstm_out, hidden = model.lstm(emb, hidden)\n",
    "            hidden_out = model.hidden2out(lstm_out)\n",
    "            softmax_out = F.log_softmax(hidden_out, dim = 2)\n",
    "            softmax_out = softmax_out[0][-1]\n",
    "            prediction = softmax_out.argmax().tolist()\n",
    "            print(int_to_vocab[prediction],end = '')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ronald Weasley, who has always been overshadonsil eyes watching down the walls. \n",
      "\n",
      "\"All right, but I warn yeh, he's a coward,\" said Hermione grimly. \"Flitwi have no fining somethin', has is but the night my parents died.\" \n",
      "\n",
      "Hagrid leaned across the table, his eyes from the ground and the knight turteing him. \n",
      "\n",
      "\"Shut up, both of you!\" said Harry sharplykon in his glass, as fast-scase-Fin a cat wand. He caught Harry's bathrobe for a while, but Snape didn't look at him again, and one of hiuse to ask you. I think Hermione does, though, which was measuring bewarely through the whirl ofing and noise. \n",
      "\n",
      "Percy was allowed to fly it, snaking all over the floor. He didn't say anything. \n",
      "\n",
      "\"Where is this school, anyway?\" \n",
      "\n",
      "\"I don't know, sit,\" said Hagrid, giving Harry another of his sideways looked just a chance or witches. \n",
      "\n",
      "It was a nice laugh at the spill show he had no fore about Snape.... \n",
      "\n",
      "Our he didn't want to talk to him anywhere fell from the corner of his eye he saw the fluttering banner high above, flashing  power. \n",
      "\n",
      "The troll stopped a few feet from Hermione. It lumbered around, blinking stupidly, to the fire in the Gry find morning he'd gotten into a heaps of things out of the classroom stood up to Professor McGonagall had used to say is for a bit of a shock and blonded and stop it  nare. He was hungry, himself. \n",
      "\n",
      "CHAPTER THIRMEN \n",
      "\n",
      "THE MIRROR OF ERISED \n",
      "\n",
      "Christmas was the only one hope that had passed, too, feet after all, and his marks weren't bad. \n",
      "\n",
      "rather Potter was here to the floor where it lay in gleaming folds. Ron was standi\" noises that he wasn't a teacher's question. She was staring at her slippopershars stumbled back into its broom, can didn't have a sister, because her sister and he stared at Harry for a few moments, then, as though he had sung something like this three hals but so worry about the Potters had a small soft of people hurrying in different directions. There was a very good chance theusand George he'd got him a bit of a sort of smile, as if ever there wasn"
     ]
    }
   ],
   "source": [
    "generateSentence('Ronald Weasley', 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
